{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias, Variance, and Model Validation\n",
    "\n",
    "![validation gif from giphy](https://media.giphy.com/media/242wLqQerWkxd6GgHB/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Define _bias_ and _variance_ in the context of machine learning\n",
    "- Recognize how bias and variance are related to under- and over-fitting\n",
    "- Summarize why validation is important\n",
    "- Describe how a train-test split works\n",
    "- Apply a train-test split to a dataset using sklearn\n",
    "\n",
    "Bonus:\n",
    "- Explain why k-fold cross validation is often more robust than a single train-test split\n",
    "- Apply k-fold cross validation to a dataset using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap what we've done so far this week, and especially to reiterate what we did yesterday, let's look at some generated data and see what adding polynomial terms does to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Setting random seed for reproducibility\n",
    "np.random.seed(1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 150 samples from uniform distribution between -2pi and 2pi\n",
    "\n",
    "x = np.random.uniform(-2*np.pi, 2*np.pi, 150)\n",
    "\n",
    "# Creating target (y) - so we know the true relationship between x and y\n",
    "# But - adding some noise (error) with 'np.random'\n",
    "\n",
    "y = np.sin(x) + np.random.normal(loc=0, scale=0.4, size=len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y)\n",
    "\n",
    "plt.ylabel('$\\sin(x)$')\n",
    "plt.xlabel('x values are randomly chosen from $[-2\\pi, 2\\pi]$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting a linear model\n",
    "lr = LinearRegression()\n",
    "lr.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grabbing the predicted values\n",
    "y_pred = lr.predict(x.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring our model\n",
    "print(f\"R2 Score: {r2_score(y, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize it\n",
    "plt.scatter(x, y) # original data\n",
    "\n",
    "plt.plot(x, y_pred, c='red') # predicted values\n",
    "\n",
    "plt.ylabel('$\\sin(x)$ + noise')\n",
    "plt.xlabel('x values randomly chosen between $-2\\pi$ and $2\\pi$')\n",
    "plt.title(\"Linear Regression\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this a good model? Well - of course not. It is what we would call **underfit** - it is not complex enough to accurately capture the pattern and predict the target.\n",
    "\n",
    "Let's try again, but now with polynomials!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this, we'll need some helper functions\n",
    "# Shoutout to Andy for sending me these\n",
    "\n",
    "def create_poly_dataset(x, degree):\n",
    "    \"\"\"\n",
    "    returning dataset with the given polynomial degree\n",
    "    \"\"\"\n",
    "    # Instantiate the PolynomialFeatures object with given 'degree'\n",
    "    poly = PolynomialFeatures(degree=degree)\n",
    "\n",
    "    # Now transform data to create higher order features\n",
    "    new_data = poly.fit_transform(x.reshape(-1, 1))\n",
    "    return new_data\n",
    "\n",
    "def fit_linear_model(data, y):\n",
    "    \"\"\"\n",
    "    fitting a linear model and printing model details\n",
    "    \"\"\"\n",
    "    np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(-1, 1)\n",
    "\n",
    "    lr = LinearRegression(fit_intercept=False)\n",
    "    lr.fit(data, y)\n",
    "    print(\"-\"*13)\n",
    "    print(\"Coefficients: \", lr.coef_)\n",
    "    y_pred = lr.predict(data)\n",
    "    print(f\"R-Squared: {lr.score(data, y):.3f}\")\n",
    "    return lr\n",
    "\n",
    "def plot_predict(x, y, model):\n",
    "    \"\"\"\n",
    "    plotting predictions against true values\n",
    "    \"\"\"\n",
    "    plt.scatter(x, y, label='true')\n",
    "    x_pred = np.linspace(x.min(), x.max(), 100)\n",
    "    \n",
    "    # visualize beyond this x range by uncommenting below:\n",
    "#     extra = x.ptp() * .2\n",
    "#     x_pred = np.linspace(x.min() - extra, x.max() + extra, 100)\n",
    "\n",
    "    plt.plot(x_pred, model.predict(create_poly_dataset(x_pred, len(model.coef_)-1)),\n",
    "             label='predicted', c='red')\n",
    "\n",
    "    if len(model.coef_) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms \\n (no slope)\")\n",
    "    elif (len(model.coef_) - 1) == 1:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Term\")\n",
    "    else:\n",
    "        plt.title(f\"{len(model.coef_) - 1} Polynomial Terms\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualizing an assortment of polynomial degrees\n",
    "# can visualize each sequential polynomial with `range(n)`\n",
    "for i in [0, 1, 2, 3, 5, 7, 9, 13, 18]:\n",
    "    xi = create_poly_dataset(x, i)\n",
    "    plot_predict(x, y, fit_linear_model(xi, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So... which one is best?\n",
    "\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Bias-Variance Trade Off\n",
    "\n",
    "<img alt=\"original image from https://rmartinshort.jimdofree.com/2019/02/17/overfitting-bias-variance-and-leaning-curves/\" src=\"images/underfit-goodfit-overfit.png\" width=750, height=350>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember - by modeling, we're assuming that there is some relationship between our X variables (the features in our dataset) and our y variable (the target). Thus, there is some underlying '_true_' function that captures the relationship between X and y, which we are trying to find by modeling. Of course, the actual relationship may be quite complex and not wholly represented in our data - our approximation, aka the model we create, is likely only a simplified estimator of whatever our '_true_' function actually would look like.\n",
    "\n",
    "**Bias**: Error introduced by approximating a real-life problem (which may be extremely complicated) by a much simpler model (because the model is too simple to capture the underlying pattern)\n",
    "\n",
    "**Variance**: Amount by which our model would change if we estimated it using a different training dataset (because the model is over-learning from the training data)\n",
    "\n",
    "**Representation:**\n",
    "\n",
    "<img alt=\"from https://hsto.org/files/281/108/1e9/2811081e9eda44d08f350be5a9deb564.png\" src=\"images/bias-variance.png/\" width=350, height=350>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How To Minimize Bias and Variance\n",
    "\n",
    "Good news! There are tried and true methods to reducing both bias and variance in our modeling process. Testing different models, trying models on different slices of data, transforming or engineering features - all of these things have a role to play in creating better, more robust models.\n",
    "\n",
    "In particular, we've learned so far that we can evaluate the performance of our models, using a scoring metric, which will help us catch if a model is underfit - if it's performing quite poorly, it probably isn't capturing the relationship in our data! \n",
    "\n",
    "But what about overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Validation\n",
    "\n",
    "Let's say you have a dataframe, with some number of rows of data, and that's all you have available to you. The hope is that you can train a model on this data that can then be used to make predictions about new data that comes in. You want your model to generalize well and work on this incoming data - not too complex from learning all the details/noise from the data, but also not so simple that the model is useless. How do we do that?\n",
    "\n",
    "<img alt=\"I Love Lucy shrug gif from Giphy\" src=\"https://media.giphy.com/media/JRhS6WoswF8FxE0g2R/giphy.gif\" width=350, height=350>\n",
    "\n",
    "### Train-Test Split\n",
    "\n",
    "The idea: don't train your model on ALL of your data, but keep some of it in reserve to test on, in order to simulate how it will work on new/incoming data.\n",
    "\n",
    "#### Example:\n",
    "\n",
    "<img alt=\"original image from https://www.dataquest.io/wp-content/uploads/kaggle_train_test_split.svg plus some added commentary\" src=\"images/traintestsplit_80-20.png\" width=850, height=150>  \n",
    "\n",
    "Note - here, it looks like we're just taking the tail end of the dataset and setting it aside. In practice (most of the time), the split will randomly choose which rows are in the train vs. test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does this fight against overfitting? By witholding data from the training process, we are testing whether the model actually _generalizes_ well. If it does poorly on the test set, it's a good sign that our model learned too much noise from the train set and is overfit! \n",
    "\n",
    "![arrested development gif, found by Andy](https://heavy.com/wp-content/uploads/2013/05/tumblr_mjm9fqhrle1rvnnvyo6_250.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice:\n",
    "\n",
    "Let's go back to our Credit data, where we are trying to predict `balance`.\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Credit.csv',\n",
    "                 usecols=['Income', 'Limit', 'Rating',\n",
    "                          'Cards', 'Age', 'Balance'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our X and y\n",
    "\n",
    "X = None\n",
    "y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split here!\n",
    "# Set test_size = .33\n",
    "# Set random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What did that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_train + X_test) == len(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put our train/test split into practice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a scaler to scale our data\n",
    "# Let's use Standard Scaler here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit our scaler - ON THE TRAINING DATA!!\n",
    "# Then transform both train and test \n",
    "\n",
    "X_train_scaled = None\n",
    "X_test_scaled = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an sklearn linear model\n",
    "\n",
    "lr = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your model - ON THE TRAINING DATA!!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab predictions for train and test set\n",
    "\n",
    "y_pred_train = None\n",
    "y_pred_test = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How'd we do?\n",
    "\n",
    "print(f\"Train R2 Score: {None}\")\n",
    "print(f\"Test R2 Score: {None}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate!\n",
    "\n",
    "- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But Wait... There's More!\n",
    "\n",
    "Let's change something and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                        test_size=0.33, \n",
    "                                                        random_state=n) # <--\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lr.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred_train = lr.predict(X_train_scaled)\n",
    "    y_pred_test = lr.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"Random Seed: {n}\")\n",
    "    print(f\"Train R2 Score: {r2_score(y_train, y_pred_train)}\")\n",
    "    print(f\"Test R2 Score: {r2_score(y_test, y_pred_test)}\")\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening here? All we're doing is changing our `random_seed` - why is that having such an impact on our model's scores? Some models appear overfit, some don't - and for some, the test score is **better** than our train score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross-Validation\n",
    "\n",
    "Sometimes, random chance means your training data isn't representative, or includes wacky data like all of our outliers. So, why do just one train-test split when you can do `k` number of them!\n",
    "\n",
    "![cross validation image from kaggle: https://www.kaggle.com/alexisbcook/cross-validation](images/cross-validation.png)\n",
    "\n",
    "The good news is, we'll never actually have to do this by hand - `sklearn` will handle it for us!\n",
    "\n",
    "Documentation: https://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale our data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a fresh linear regression model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use cross_val_score\n",
    "# Set cv = 5\n",
    "\n",
    "scores = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the test scores across our folds\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print it nicely\n",
    "print(f\"Scores: {scores.mean():.3f} +/- {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why show the standard deviation of scores here? I want some measure of the variance among my scores, so I can tell how different my scores were based on different breakdowns of the training data.\n",
    "\n",
    "If I made a change to my model and the average of my cross-validated scores stayed about the same, but the variance among those scores decreased, that's a better, more generalizable model than before!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources:\n",
    "\n",
    "- [Great bias/variance infographic](https://elitedatascience.com/bias-variance-tradeoff) from Elite Data Science"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
